import numpy as np
import torch
import torch.nn as nn
from torch.nn import init
import torch.optim as optim
import math
import random
import os
import time
from tqdm import tqdm
import json
import string
import pickle

unk = '<UNK>'

class RNN(nn.Module):
    def __init__(self, input_dim, h):  
        super(RNN, self).__init__()
        self.h = h
        self.numOfLayer = 1
        self.rnn = nn.RNN(input_dim, h, self.numOfLayer, nonlinearity='tanh',num_layers=2, dropout=0.3)
        self.W = nn.Linear(h, 5)
        self.softmax = nn.LogSoftmax(dim=1)
        self.loss = nn.NLLLoss()

    def compute_Loss(self, predicted_vector, gold_label):
        return self.loss(predicted_vector, gold_label)

    def forward(self, inputs):
        output_seq, hidden = self.rnn(inputs)             
        output_scores = self.W(output_seq)                
        summed_scores = torch.sum(output_scores, dim=0)   
        predicted_vector = self.softmax(summed_scores)    
        return predicted_vector
print(f"Training data size: {len(train_data)}")
print(f"Validation data size: {len(valid_data)}")
print(f"Test data size: {len(test_data)}")
print(f"Training samples: {len(train_data)}, Batch size: {minibatch_size}")
print(f"Expected batches per epoch: {len(train_data) // minibatch_size}")


def load_data(train_data, val_data, test_data):
    with open(train_data) as training_f:
        training = json.load(training_f)
    with open(val_data) as valid_f:
        validation = json.load(valid_f)
    with open(test_data) as test_f:
        test = json.load(test_f)

    tra = []
    val = []
    tst = []
    
    for elt in training:
        tra.append((elt["text"].split(), int(elt["stars"] - 1)))
    for elt in validation:
        val.append((elt["text"].split(), int(elt["stars"] - 1)))
    for elt in test:
        tst.append((elt["text"].split(), int(elt["stars"] - 1)))
    
    return tra, val, tst


if __name__ == "__main__":
    hidden_dim = 64  
    epochs = 10  
    train_data_path = "/content/drive/MyDrive/Colab Notebooks/training.json"
    val_data_path = "/content/drive/MyDrive/Colab Notebooks/validation.json"
    test_data_path = "/content/drive/MyDrive/Colab Notebooks/test.json"  

    print("========== Loading data ==========")
    train_data, valid_data, test_data = load_data(train_data_path, val_data_path, test_data_path)

    print("========== Vectorizing data ==========")
    model = RNN(50, hidden_dim)  
    optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-5)
    word_embedding = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/word_embedding.pkl', 'rb'))

    stopping_condition = False
    epoch = 0

    last_train_accuracy = 0
    last_validation_accuracy = 0

    while not stopping_condition and epoch < epochs:  
        random.shuffle(train_data)
        model.train()
        print("Training started for epoch {}".format(epoch + 1))

        correct = 0
        total = 0
        minibatch_size = 8
        N = len(train_data)
        loss_total = 0
        loss_count = 0

        #for minibatch_index in tqdm(range(N // minibatch_size)):
        for minibatch_index in tqdm(range((N + minibatch_size - 1) // minibatch_size)):  # Ensure all samples are covered

            optimizer.zero_grad()
            loss = None
            for example_index in range(minibatch_size):
                input_words, gold_label = train_data[minibatch_index * minibatch_size + example_index]
                input_words = " ".join(input_words)
                input_words = input_words.translate(input_words.maketrans("", "", string.punctuation)).split()
                vectors = [word_embedding[i.lower()] if i.lower() in word_embedding.keys() else word_embedding['unk'] for i in input_words]
                vectors = torch.tensor(vectors).view(len(vectors), 1, -1)
                output = model(vectors)

                example_loss = model.compute_Loss(output.view(1, -1), torch.tensor([gold_label]))

                predicted_label = torch.argmax(output)
                correct += int(predicted_label == gold_label)
                total += 1

                if loss is None:
                    loss = example_loss
                else:
                    loss += example_loss

            loss = loss / minibatch_size
            loss_total += loss.data
            loss_count += 1
            loss.backward()
            optimizer.step()

        print(loss_total / loss_count)
        print("Training completed for epoch {}".format(epoch + 1))
        print("Training accuracy for epoch {}: {}".format(epoch + 1, correct / total))
        trainning_accuracy = correct / total

        model.eval()
        correct = 0
        total = 0
        random.shuffle(valid_data)
        print("Validation started for epoch {}".format(epoch + 1))

        for input_words, gold_label in tqdm(valid_data):
            input_words = " ".join(input_words)
            input_words = input_words.translate(input_words.maketrans("", "", string.punctuation)).split()
            vectors = [word_embedding[i.lower()] if i.lower() in word_embedding.keys() else word_embedding['unk'] for i in input_words]
            vectors = torch.tensor(np.array(vectors), dtype=torch.float32).view(len(vectors), 1, -1)
            output = model(vectors)
            predicted_label = torch.argmax(output)
            correct += int(predicted_label == gold_label)
            total += 1

        print("Validation completed for epoch {}".format(epoch + 1))
        print("Validation accuracy for epoch {}: {}".format(epoch + 1, correct / total))
        validation_accuracy = correct / total

        if validation_accuracy < last_validation_accuracy and trainning_accuracy > last_train_accuracy:
            stopping_condition = True
            print("Training stopped to avoid overfitting!")
            print("Best validation accuracy:", last_validation_accuracy)
        else:
            last_validation_accuracy = validation_accuracy
            last_train_accuracy = trainning_accuracy

        epoch += 1

    # **Test Accuracy Computation**
    correct = 0
    total = 0
    print("========== Evaluating on Test Data ==========")

    for input_words, gold_label in tqdm(test_data):
        input_words = " ".join(input_words)
        input_words = input_words.translate(input_words.maketrans("", "", string.punctuation)).split()
        vectors = [word_embedding[i.lower()] if i.lower() in word_embedding.keys() else word_embedding['unk'] for i in input_words]
        vectors = torch.tensor(vectors).view(len(vectors), 1, -1)
        output = model(vectors)
        predicted_label = torch.argmax(output)
        correct += int(predicted_label == gold_label)
        total += 1

    test_accuracy = correct / total
    print("========== Test Accuracy ==========")
    print(f"Final Test Accuracy: {test_accuracy:.4f}")
